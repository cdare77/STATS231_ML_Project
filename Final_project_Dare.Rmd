---
title: "Predicting the Price of Semiconductor Stocks"
author: Chris Dare
output: 
  html_document:
    theme: united
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: no
date: "2024-05-10"
---

# Introduction

With the growing presence of technology in our society, there is an rapidly increasing demand for hardware which supports our heavy computational demands. One of the most important pieces of computer hardware for computationally-intensive tasks is Graphics Processing Units (GPU) due to their ability to handle a wide range of parallel processing tasks — this has made them an invaluable resource for companies pursing any sort of Artificial Intelligence (AI), super-computing, crypto-currencies, or computer graphics. Unfortunately, the materials needed to produce GPUs are somewhat scarce, thus leading to a small pool of manufacturers that experience significant competition.

The purpose of this project is to attempt to predict the price-trends of a fixed semiconductor stock (in this case, that of NVIDIA) based on the performance of its competitors, previous pricing, and volume of shares sold. A variety of statistical learning models will be used, ranging from standard regression techniques to more non-linear models like random forest learning and k-Nearest neighbors. 



# Loading Packages and Data

```{r setup, include=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(discrim)
library(ggthemes)
library(kableExtra)
library(yardstick)
library(visdat)
library(scales)
library(glmnet)
library(quantmod)
library(dygraphs)
library(tidyr)


tidymodels_prefer()
conflicted::conflicts_prefer(yardstick::rsq)
conflicted::conflicts_prefer(dplyr::lag)
set.seed(3435)
```

This dataset is comprised of 1 year's worth ( 252 business / trading days ) of New York Stock Exchange data for twelve of the most popular semiconductor manufacturers: Advanced Micro Devices (AMD), Applied Materials Inc. (AMAT), ASML Holding N.V. (ASML), Broadcom Inc. (AVGO), Intel Corporation (INTC), Monolithic Power Systems Inc. (MPWR), Nvidia Corp. (NVDA), NXP Semiconductors NV (NXPI), On Semiconductor Corp. (ON), Qualcomm Inc. (QCOM), Taiwan Semiconductor Manufacturing Co. Ltd. (TSM), and Texas Instruments Inc (TXN).

All stock market data was obtained from [Yahoo Finance](https://finance.yahoo.com/). Each company's one-year historical stock data was individually pulled from Yahoo's historical data on April 12th, 2024. For example, AMD's stock prices were obtained by downloading the CSV file from [AMD's Historical Data page](https://finance.yahoo.com/quote/AMD/history/), which results in a dataframe with the following variables and entries:

```{r  eval = TRUE, include=TRUE, warnings=FALSE}
loadSymbols("AMD", from="2023-01-01")
chartSeries(AMD,TA=c(addVo(),addBBands(),addMACD()))
my_dates <- index(AMD)
AMD <- data.frame(AMD)
AMD$AMD.Date <- my_dates
```



However, since one goal of this analysis is to test the affect of competitor's stock performance on a fixed GPU manufacturer's stock price, multiple CSV files must be stored into raw data. The easiest way to do this was to create a separate CSV file, with header columns renamed to both resolve variable name conflicts and to distinguish the data specific to certain stocks. This was simply done by adding the stock's symbol (i.e. AMD, INTC, etc.) to the beginning of the original variable name: 



This results in a seemingly large initial data frame that contains different 73 predictors and 252 entries (corresponding to the 252 days that the stock market is open throughout the fiscal year). Out of the 73 predictors is 1 date variable (which is formatted as a Date data type using `as.Date()` ), and 6 predictors for each of the 12 chosen semiconductor manufacturers.

```{r  eval = TRUE, include=TRUE, warnings=FALSE}
dim(AMD)
```

Fortunately, a quick analysis shows that there is no missing data among any of the CSV files downloaded. This is somewhat expected though, since stock market data is meant to be as publicly available as possible and the original features are fairly common metrics for financial institutions to collect.

```{r  eval = TRUE, include=TRUE, warnings=FALSE}
vis_miss(AMD)
```


# Exploratory Data Analysis

We examine the data in terms of the predictors that are given to us, and then see if there are any other possible metrics to analyze our stock prices by. First we wish to explain the relevance of each predictor in the initial data frame, though not all variables will be used in our predictive models due to a high correlation (for example, the previous day's closing price is heavily correlated to the current day's opening price). Next, we examine other possible methods to predict our stocks behavior by looking at both historical metrics and normalized metrics. 

## Describing the Predictors

* `Open`: 
* `Close`:
* `Low`:
* `High`:
* `Volume`:

For each of the 12 semiconductor manufacturers chosen (AMAT, AMD, ASML, AVGO, INTC, MPWR, NVDA, NXPI, ON, QCOM, TSM, and TXN), there is a variable called xx_Open (where xx is one of the above stock symbols) which corresponds to that stock's opening price for the day. As the New York Stock exchange operates from 9:30AM to 4:00PM, this indicates the stock's price at 9:30AM that day. 

```{r include = TRUE, warning = FALSE, message = FALSE }
p <- dygraph(AMD[,c(1:4)], xlab = "Date", ylab = "Price", main = "AMD Price") %>%
  dySeries("AMD.Open", label = "Open", color = "black") %>%
  dySeries("AMD.Low", label = "Low", color = "red") %>%
  dySeries("AMD.High", label = "High", color = "green") %>%
  dySeries("AMD.Close", label = "Close", color = "orange") %>%
  dyRangeSelector() %>%
  dyCandlestick()%>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = T)  %>%
  dyRoller(rollPeriod = 1)
p
```



```{r include = TRUE, warning = FALSE, message = FALSE }
AMD$AMD.Close_L1 <- lag(AMD$AMD.Close, 1)
AMD$AMD.Close_L2 <- lag(AMD$AMD.Close, 2)
AMD$AMD.Close_L3 <- lag(AMD$AMD.Close, 3)
AMD$AMD.Close_L4 <- lag(AMD$AMD.Close, 4)
AMD$AMD.Close_L5 <- lag(AMD$AMD.Close, 5)
AMD$AMD.Close_L6 <- lag(AMD$AMD.Close, 6)
AMD$AMD.Close_L7 <- lag(AMD$AMD.Close, 7)
AMD$AMD.Close_L8 <- lag(AMD$AMD.Close, 8)
AMD$AMD.Close_L9 <- lag(AMD$AMD.Close, 9)
AMD$AMD.Close_L10 <- lag(AMD$AMD.Close, 10)
AMD$AMD.Close_L11 <- lag(AMD$AMD.Close, 11)
AMD$AMD.Close_L12 <- lag(AMD$AMD.Close, 12)
AMD$AMD.Close_L13 <- lag(AMD$AMD.Close, 13)
AMD$AMD.Close_L14 <- lag(AMD$AMD.Close, 14)


AMD <- AMD %>%
  fill(AMD.Close_L1, .direction = "up") %>%
  fill(AMD.Close_L2, .direction = "up") %>%
  fill(AMD.Close_L3, .direction = "up") %>% 
  fill(AMD.Close_L4, .direction = "up") %>% 
  fill(AMD.Close_L5, .direction = "up") %>% 
  fill(AMD.Close_L6, .direction = "up") %>% 
  fill(AMD.Close_L7, .direction = "up") %>% 
  fill(AMD.Close_L8, .direction = "up") %>% 
  fill(AMD.Close_L9, .direction = "up") %>% 
  fill(AMD.Close_L10, .direction = "up") %>% 
  fill(AMD.Close_L11, .direction = "up") %>%
  fill(AMD.Close_L12, .direction = "up") %>%
  fill(AMD.Close_L13, .direction = "up") %>% 
  fill(AMD.Close_L14, .direction = "up") 
```
 
## Added Predictors and Metrics

While the six predictors provided by Yahoo Finance give significant insight into each stock's historical performance over the year, there may be other, more useful metrics that we can use to assess and predict the future growth of our stocks. The main kind of variables we wish to introduce are ones which simply keep track of data from previous days; since each predictor in our original data frame only applies to a 24-hour window, there could be some potentially important information in the long-term trends of a stock which ultimately affect a share's price.


### Simple Moving Average of Closing Price

As consumers use historical stock data to determine whether a certain stock is worth buying or not, it becomes apparent that stocks' price is, in one way or another, dependent on its previous value. While this is technically true for any continuous function / continuous random variable, it is clear that even long-term data can affect a stock's current value — for example, if a stock has been in a steady downward trend for quite some time, it will negatively affect the perception of potential investors. 

While there are multiple financial metrics which account for previous stock prices, this analysis will only look at two basic measurements: the simple moving average and the simple moving standard deviation (where n is some integer-valued hyper-parameter). Although there are subtle differences between the opening price and the closing price of a stock, the larger the moving value is (i.e. the number of days averaged over) the less the distinction should matter in terms of which variable to average; for consistency, we will simply base our new metrics on the closing costs of each stock.

Additionally, there is no clear choice for how much previous data to account for — should the analysis look back at a single week's worth of data or a month? As this is itself an interesting question for the sake of tuning our models, **we will consider this an added hyperparameter for the problem** and consider four possible values: 1 week, 2 weeks, 1 month, and 2 months.

```{r  eval = TRUE, include=TRUE, warnings=FALSE }

simple_moving_average <- function(my_vec, lag_period) {
  #' Takes the running average of a column vector
  #'
  #' Creates a new column vector whose entries are the average of the previous lag_period entries.
  #' When not enough data is available to take the average over lag_period, the closest possible 
  #' average will be taken (for example, if lag_period = 10, then the first 2nd entry of the output
  #' vector will simply be the average of the first two values, the 3rd entry of the output vector
  #' will be the average of the first three values, and so forth.)
  #'
  #' @param my_vec the column vector to take the average values of
  #' @param lag_period the number of days one wishes to average over
  #' 
  #' @return A vector whose entries represent the average of the previous lag_period entries in my_vec
  
  
  # Error handling
  if(is.vector(my_vec) == FALSE){
    stop("Not Vector: First argument of simple_moving_average must be a vector")
  }
  if(is.numeric(my_vec[1]) == FALSE){
    stop("Non-numeric Entries: values of vector in first argument must be numeric.")
  }
  if(is.numeric(lag_period) == FALSE || lag_period != round(lag_period)){
    stop("Not Integer: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }
  if(lag_period <= 1){
    stop("Not Large Enough: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }
 
  # return variable
  output_vec = c()
  for (i in 1:length(my_vec)) {
    # If there are less that lag_period of data previous to the current date,
    # simply take the average of all the days prior to get the closest thing
    # to a running average
    if (i <= lag_period){
      output_vec[i] = mean(my_vec[1:i])
    }
    else {
      output_vec[i] = mean(my_vec[(i-lag_period + 1):i])
    }
  }
  return(output_vec)
}
  
AMD$AMD.SMA_cl_1W <- simple_moving_average(AMD$AMD.Close, 5)
AMD$AMD.SMA_cl_2W <- simple_moving_average(AMD$AMD.Close, 10)
AMD$AMD.SMA_cl_1M <- simple_moving_average(AMD$AMD.Close, 20)
AMD$AMD.SMA_cl_2M <- simple_moving_average(AMD$AMD.Close, 40)


ggplot(data = AMD, aes(x=AMD.Date)) +
  geom_line(aes(y = AMD.SMA_cl_1W, color = '1-Week')) + 
  geom_line(aes(y = AMD.SMA_cl_2W, color = '2-Week')) +
  geom_line(aes(y = AMD.SMA_cl_1M, color = '1-Month')) +
  geom_line(aes(y = AMD.SMA_cl_2M, color = '2-Month')) +
  ylab('USD') +
  scale_color_manual(values = c(
    '1-Week' = 'firebrick1',
    '2-Week' = 'chocolate1',
    '1-Month' = 'gold',
    '2-Month' = 'chartreuse'
  )) +
  xlab('Date') +
  ggtitle("Simple Moving Average") +
  scale_y_continuous(limits = c(0, 220), labels = label_comma()) +
  theme_dark()
```

One characteristic that immediately becomes apparent is that evaluating the running averages instead of the closing costs seems to "smooth out" the curves — in other words, the running average is much more stable and is not affected by a share's volatility as much as our original predictors obtained from the CSV. In fact, what we are actually doing is slowly interpolating the data with the overall average; since the overall average is a constant function (and thus linear), the "smoothing out" process is simply a result of interpolating with a $C^\infty(\mathbb{R})$ (smooth) function.



 
### Exponential Moving Average

```{r  eval = TRUE, include=TRUE, warnings=FALSE }
exponential_moving_average_helper <- function(my_vec, lag_period, smoothing_factor) {
  #' Helper function to evaluate the exponential moving average over a fixed period 
  #' using an array buffer
  #'
  #' Creates a new column vector whose entries are the average of the previous lag_period entries.
  #' When not enough data is available to take the average over lag_period, the closest possible 
  #' average will be taken (for example, if lag_period = 10, then the first 2nd entry of the output
  #' vector will simply be the average of the first two values, the 3rd entry of the output vector
  #' will be the average of the first three values, and so forth.)
  #'
  #' @param my_vec the column vector to take the average values of
  #' @param lag_period the number of days one wishes to average over
  #' 
  #' @return A vector whose entries represent the average of the previous lag_period entries in my_vec
  
  
  # Error handling
  if(is.vector(my_vec) == FALSE){
    stop("Not Vector: First argument of simple_moving_average must be a vector")
  }
  if(is.numeric(my_vec[1]) == FALSE){
    stop("Non-numeric Entries: values of vector in first argument must be numeric.")
  }
  if(is.numeric(lag_period) == FALSE || lag_period != round(lag_period)){
    stop("Not Integer: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }
  if(lag_period <= 1){
    stop("Not Large Enough: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }
 
  vec_len <- length(my_vec)
  output_vec <- c()
  output_vec[1] <- my_vec[1]
  
  for (i in 2:vec_len) {
    output_vec[i] <- smoothing_factor * my_vec[i] + (1 - smoothing_factor) * output_vec[i-1]
  }
  
  return(output_vec[vec_len])
}

exponential_moving_average <- function(my_vec, lag_period) {
  #' Takes the running average of a column vector
  #'
  #' Creates a new column vector whose entries are the average of the previous lag_period entries.
  #' When not enough data is available to take the average over lag_period, the closest possible 
  #' average will be taken (for example, if lag_period = 10, then the first 2nd entry of the output
  #' vector will simply be the average of the first two values, the 3rd entry of the output vector
  #' will be the average of the first three values, and so forth.)
  #'
  #' @param my_vec the column vector to take the average values of
  #' @param lag_period the number of days one wishes to average over
  #' 
  #' @return A vector whose entries represent the average of the previous lag_period entries in my_vec
  
  
  # Error handling
  if(is.vector(my_vec) == FALSE){
    stop("Not Vector: First argument of simple_moving_average must be a vector")
  }
  if(is.numeric(my_vec[1]) == FALSE){
    stop("Non-numeric Entries: values of vector in first argument must be numeric.")
  }
  if(is.numeric(lag_period) == FALSE || lag_period != round(lag_period)){
    stop("Not Integer: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }
  if(lag_period <= 1){
    stop("Not Large Enough: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }
 
  output_vec <- c()
  output_vec[1] <- my_vec[1]
  smoothing_factor = 2/(lag_period+1)
  for (i in 2:length(my_vec)) {
    # If there are less that lag_period of data previous to the current date,
    # simply take the average of all the days prior to get the closest thing
    # to a running average
    if (i <= lag_period){
      output_vec[i] = exponential_moving_average_helper(my_vec[1:i], i, 2/(i + 1))
    }
    else {
      output_vec[i] = exponential_moving_average_helper(my_vec[(i-lag_period + 1):i], lag_period, smoothing_factor)
    }
  }
  
  return(output_vec)
}


AMD$AMD.EMA_cl_1W <- exponential_moving_average(AMD$AMD.Close, 5)
AMD$AMD.EMA_cl_2W <- exponential_moving_average(AMD$AMD.Close, 10)
AMD$AMD.EMA_cl_1M <- exponential_moving_average(AMD$AMD.Close, 20)
AMD$AMD.EMA_cl_2M <- exponential_moving_average(AMD$AMD.Close, 40)

ggplot(data = AMD, aes(x=AMD.Date)) +
  geom_line(aes(y = AMD.EMA_cl_1W, color = '1-Week')) + 
  geom_line(aes(y = AMD.EMA_cl_2W, color = '2-Week')) +
  geom_line(aes(y = AMD.EMA_cl_1M, color = '1-Month')) +
  geom_line(aes(y = AMD.EMA_cl_2M, color = '2-Month')) +
  ylab('USD') +
  scale_color_manual(values = c(
    '1-Week' = 'firebrick1',
    '2-Week' = 'chocolate1',
    '1-Month' = 'gold',
    '2-Month' = 'chartreuse'
  )) +
  xlab('Date') +
  ggtitle("Exponential Moving Average") +
  scale_y_continuous(limits = c(0, 220), labels = label_comma()) +
  theme_dark()
```





### Simple Moving Deviation of Closing Price

With a concrete notion of the simple moving average closing price of a stock, it is natural to measure the standard deviation as well to gain an accurate insight on the volatility of each stock.

```{r  eval = TRUE, include=TRUE, warnings=FALSE }
simple_moving_deviation <- function(my_vec, lag_period) {
  #' Takes the running standard deviation of a column vector
  #'
  #' Creates a new column vector whose entries are the standard deviation of the previous lag_period entries.
  #' When not enough data is available to take the deviation over lag_period, the closest possible 
  #' average will be taken (for example, if lag_period = 10, then the first 2nd entry of the output
  #' vector will simply be the average of the first two values, the 3rd entry of the output vector
  #' will be the average of the first three values, and so forth.)
  #'
  #' @param my_vec the column vector to take the standard deviation of
  #' @param lag_period the number of days one wishes to average over
  #' 
  #' @return A vector whose entries represent the standard deviation of the previous lag_period entries in my_vec
  
  
  # Error handling
  if(is.vector(my_vec) == FALSE){
    stop("Not Vector: First argument of simple_moving_average must be a vector")
  }
  if(is.numeric(my_vec[1]) == FALSE){
    stop("Non-numeric Entries: values of vector in first argument must be numeric.")
  }
  if(is.numeric(lag_period) == FALSE || lag_period != round(lag_period)){
    stop("Not Integer: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }
  if(lag_period <= 1){
    stop("Not Large Enough: Second argument of simple_moving_average must be an integer larger than or equal to 2")
  }

  # return variable
  output_vec = c()
  
  # Setting the first standard deviation to 0 and beginning the loop
  # at 2 prevents a divide by 0 error without adding an additional if-else branch
  # in the loop
  output_vec[1] = 0
  for (i in 2:length(my_vec)) {
    
    # If there are less that lag_period of data previous to the current date,
    # simply take the average of all the days prior to get the closest thing
    # to a running average
    if (i <= lag_period){
       output_vec[i] = sd(my_vec[1:i])
    }
    else {
      output_vec[i] = sd(my_vec[(i-lag_period+1):i])
    }
  }
  return(output_vec)
}


AMD$AMD.SMD_cl_1W <- simple_moving_deviation(AMD$AMD.Close, 5)
AMD$AMD.SMD_cl_2W <- simple_moving_deviation(AMD$AMD.Close, 10)
AMD$AMD.SMD_cl_1M <- simple_moving_deviation(AMD$AMD.Close, 20)
AMD$AMD.SMD_cl_2M <- simple_moving_deviation(AMD$AMD.Close, 40)

ggplot(data = AMD, aes(x=AMD.Date)) +
  geom_line(aes(y = AMD.SMD_cl_1W, color = '1-Week')) + 
  geom_line(aes(y = AMD.SMD_cl_2W, color = '2-Week')) +
  geom_line(aes(y = AMD.SMD_cl_1M, color = '1-Month')) +
  geom_line(aes(y = AMD.SMD_cl_2M, color = '2-Month')) +
  ylab('USD') +
  scale_color_manual(values = c(
    '1-Week' = 'firebrick1',
    '2-Week' = 'chocolate1',
    '1-Month' = 'gold',
    '2-Month' = 'chartreuse'
  )) +
  xlab('Date') +
  ggtitle("Simple Moving Deviation") +
  scale_y_continuous(limits = c(0, 25), labels = label_comma()) +
  theme_dark()
```



### Moving Average Convergence Divergence (MACD)


```{r  eval = TRUE, include=TRUE, warnings=FALSE }
AMD$AMD.MACD = (AMD$AMD.EMA_cl_2W - AMD$AMD.EMA_cl_1M)

ggplot(data = AMD, aes(x=AMD.Date)) +
  geom_line(aes(y = AMD.MACD, color = 'MACD')) +
  ylab('USD') +
  scale_color_manual(values = c(
    'MACD' = 'aquamarine'
  )) +
  xlab('Date') +
  ggtitle("MACD Line") +
  scale_y_continuous(limits = c(-10, 10), labels = label_comma()) +
  theme_dark()
```


## Data Correlation

While having a large array of predictors is in some sense useful for seeing the whole picture of the semiconductor market for the 2023-2024 fiscal year, there is also a potentially significant amount of unnecessary information. As mentioned prior, the behavior of many of our initial predictors coming from the CSV files are very closely related to one another — the closing price one day is directly tied to the opening price of the following day, and if a stock's minimum / Low value is increasing that generally means all 4 other predictors (aside from volume) are increasing as well. In addition, comparing the performance between two stocks is generally going to be heavily correlated due to the fact that they both follow the underlying market's climate.

Ultimately, in order to achieve a good understanding of the correlations between all of our predictors we will need to cross examine several subsets of our predictors to see which predictors are correlated for a single stock, and which predictors are useful for measuring competition between stocks. Dividing our correlation plots into two types, we first examine how the predictors are correlated for a **fixed** stock, and test this underlying trend accross a subset of our stocks (ASML, INTC, NVDA, and NXPI ):

```{r include = TRUE, warning = FALSE, message = FALSE }
select(AMD, is.numeric) %>%
  cor() %>%
  corrplot(method = "circle", type = "lower", diag = FALSE, tl.cex=0.6, title="Correlation Plot")
```
 

 


# Setting Up Models

With a better picture in mind of how our stock prices can be measured from both the given metrics and how they interact with one another, we can now set up our data and begin training our models. This will be done in several steps, first preparing the data to ensure that our models do not become over-fitted to a particular data-set.


### Data Split

One of the primary ways we ensure robustness of our models is by partitioning our data into training and testing data. Foremost, this ensures that our model does not become overfit to the details and noise of our underlying data-set by introducing a portion of the data which is unseen during the training phase (i.e. the testing data). Ultimately, one would want outcome variable to have similar statistics / variance across both the training and testing sets — this is accomplished by *stratifying* our split about the desired outcome variable.


```{r eval = TRUE, include=TRUE, warnings=FALSE}
AMD_split <- initial_split(AMD, prop = 0.7,
                                strata = AMD.Close)
AMD_train <- training(AMD_split)
AMD_test <- testing(AMD_split)

```


## One-Week Model Fitting

```{r eval = TRUE, include=TRUE, warnings=FALSE}

AMD_recipe = recipe(AMD.Close ~ AMD.Close_L1 + AMD.Close_L2 + AMD.Close_L3 + AMD.Close_L4 +
                      AMD.Close_L5 + AMD.Close_L6 + AMD.Close_L7 + AMD.Close_L8 + AMD.Close_L9 +
                      AMD.Close_L10 + AMD.Close_L11 + AMD.Close_L12 + AMD.Close_L13 + AMD.Close_L14 +
                      AMD.SMD_cl_1W + AMD.SMA_cl_1W + AMD.MACD,
                    data=AMD_train) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```



## k-Fold Cross Validation

```{r eval = TRUE, include=TRUE, warnings=FALSE}
AMD_folds  <- vfold_cv(AMD_train, v = 10, strata = AMD.Close)
```



## Fitting the Models

```{r eval = TRUE, include=TRUE, warnings=FALSE}
# Linear Regression
lm_model <- linear_reg() %>%
  set_engine("lm")


# Ridge Regression
ridge_model <- linear_reg(mixture = 0,
                         penalty = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

# Lasso Regression
lasso_model <- linear_reg(mixture = 1,
                         penalty = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")


# Elastic Net
elastic_net_model <- linear_reg(mixture = tune(),
                              penalty = tune()) %>%
  set_mode("regression") %>%
  set_engine("glmnet")

# k-Nearest Neighbors
knn_model <- nearest_neighbor(neighbors = tune()) %>%
  set_engine("kknn") %>%
  set_mode("regression")
```


### Set Up Workflows 

```{r eval = TRUE, include=TRUE, warnings=FALSE}
# Linear Regression Workflows
lm_wflow <- workflow() %>%
  add_model(lm_model) %>%
  add_recipe(AMD_recipe)
 
# Ridge Regression Workflows
ridge_wflow <- workflow() %>%
  add_model(ridge_model) %>%
  add_recipe(AMD_recipe)
 
# Lasso Regression Workflows
lasso_wflow <- workflow() %>%
  add_model(lasso_model) %>%
  add_recipe(AMD_recipe)
 
# Elastic Net Workflows
elastic_net_wflow <- workflow() %>%
  add_model(elastic_net_model) %>%
  add_recipe(AMD_recipe)
  
# k-Nearest Neighbors Workflows
knn_wflow <- workflow() %>%
  add_model(knn_model) %>%
  add_recipe(AMD_recipe)
```



### Hyperparameter Tuning 


Set up Grids:
```{r eval = TRUE, include=TRUE, warnings=FALSE}
# Grid for Ridge Regression and Lasso Regression
no_mixture_grid <- grid_regular(penalty(range = c(0,1)), levels = 50)

# Grid for Elastic Net
elastic_net_grid <- grid_regular(penalty(range = c(0, 1),
                                     trans = identity_trans()),
                        mixture(range = c(0, 1)),
                             levels = 10)

# k-Nearest Neighbors Net
knn_grid <- grid_regular(neighbors(range = c(2,20)), levels = 10)
```




```{r eval = TRUE, include=TRUE, warnings=FALSE}

# Find optimal parameters for ridge regression
ridge_tune <- tune_grid(
  ridge_wflow,
  resamples = AMD_folds,
  grid = no_mixture_grid
)
ridge_final_wflow  <- select_best(ridge_tune , metric="rmse" ) %>%
  finalize_workflow(x=ridge_wflow )

# Find optimal parameters for lasso regression
lasso_tune  <- tune_grid(
  lasso_wflow ,
  resamples = AMD_folds,
  grid = no_mixture_grid
)
lasso_final_wflow  <- select_best(lasso_tune , metric="rmse") %>%
  finalize_workflow(x=lasso_wflow )

# Find optimal parameters for Elastic Net
elastic_net_tune  <- tune_grid(
  elastic_net_wflow ,
  resamples = AMD_folds,
  grid = elastic_net_grid
)
elastic_net_final_wflow  <- select_best(elastic_net_tune , metric = "rmse") %>%
  finalize_workflow(x=elastic_net_wflow )

# Find optimal parameters for k-Nearest Neighbors
knn_tune  <- tune_grid(
    knn_wflow ,
    resamples = AMD_folds,
    grid = knn_grid
)
knn_final_wflow  <- select_best(knn_tune , metric = "rmse") %>%
  finalize_workflow(x=knn_wflow )
```

### Model Fitting


```{r eval = TRUE, include=TRUE, warnings=FALSE}
# Linear Regression Fits
lm_fit  <- fit(lm_wflow, AMD_train)
 
# Ridge Regression Fits
ridge_fit  <- fit(ridge_final_wflow , AMD_train)
 
# Lasso Regression Fits
lasso_fit  <- fit(lasso_final_wflow , AMD_train)

# Elastic Net Fits
elastic_net_fit <- fit(elastic_net_final_wflow , AMD_train)
 
# k-Nearest Neighbors Fit
knn_fit  <- fit(knn_final_wflow , AMD_train)
```

### Model Results for predicting 1-Week Average of NVIDIA Stock Price

```{r eval = TRUE, include=TRUE, warnings=FALSE}
# Linear Regression Training
lm_train_res  <- predict(lm_fit , new_data = AMD_train %>% select(-AMD.Close))
lm_train_res  <- bind_cols(lm_train_res , AMD_train %>% select(AMD.Close))
 
# Ridge Regression Training
ridge_train_res  <- predict(ridge_fit , new_data = AMD_train %>% select(-AMD.Close))
ridge_train_res  <- bind_cols(ridge_train_res , AMD_train %>% select(AMD.Close))
 
# Lasso Regression Training
lasso_train_res  <- predict(lasso_fit , new_data =  AMD_train  %>% select(-AMD.Close))
lasso_train_res  <- bind_cols(lasso_train_res ,  AMD_train  %>% select(AMD.Close))

# Elastic Net Training
elastic_net_train_res  <- predict(elastic_net_fit , new_data =  AMD_train  %>% select(-AMD.Close ))
elastic_net_train_res  <- bind_cols(elastic_net_train_res ,  AMD_train  %>% select(AMD.Close ))

# k-Nearest Neighbors Training
knn_train_res  <- predict(knn_fit , new_data =  AMD_train  %>% select(-AMD.Close ))
knn_train_res  <- bind_cols(knn_train_res ,  AMD_train  %>% select(AMD.Close ))
```


## Model Accuracies


Root Mean Square Error (RMSE) results:
```{r eval = TRUE, include=TRUE, warnings=FALSE}
tibble(Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", "Elastic Net", "k-Nearest Neighbors"),
       One_Week = c((lm_train_res  %>% rmse( AMD.Close, .pred))$.estimate,
                    (ridge_train_res  %>% rmse( AMD.Close, .pred))$.estimate,
                    (lasso_train_res  %>% rmse( AMD.Close, .pred))$.estimate,
                    (elastic_net_train_res  %>% rmse( AMD.Close, .pred))$.estimate,
                    (knn_train_res  %>% rmse( AMD.Close, .pred))$.estimate ) 
       ) %>%
  kable() %>%
  kable_styling(full_width = F) %>%
  scroll_box(width = "100%", height = "200px")
```


R^2 results:
```{r eval = TRUE, include=TRUE, warnings=FALSE}

tibble(Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", "Elastic Net", "k-Nearest Neighbors"),
       One_Week = c((lm_train_res  %>% rsq( AMD.Close, .pred))$.estimate,
                    (ridge_train_res  %>% rsq( AMD.Close, .pred))$.estimate,
                    (lasso_train_res  %>% rsq( AMD.Close, .pred))$.estimate,
                    (elastic_net_train_res  %>% rsq( AMD.Close, .pred))$.estimate,
                    (knn_train_res  %>% rsq( AMD.Close, .pred))$.estimate ) 
       ) %>%
  kable() %>%
  kable_styling(full_width = F) %>%
  scroll_box(width = "100%", height = "200px")
```