---
title: "Predicting the Price of Semiconductor Stocks"
author: Chris Dare
output: 
  html_document:
    theme: united
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: no
date: "2024-05-10"
---

# Introduction

With the growing presence of technology in our society, there is an rapidly increasing demand for hardware which supports our heavy computational demands. One of the most important pieces of computer hardware for computationally-intensive tasks is Graphics Processing Units (GPU) due to their ability to handle a wide range of parallel processing tasks — this has made them an invaluable resource for companies pursing any sort of Artificial Intelligence (AI), super-computing, crypto-currencies, or computer graphics. Unfortunately, the materials needed to produce GPUs are somewhat scarce, thus leading to a small pool of manufacturers that experience significant competition.

The purpose of this project is to attempt to predict the price-trends of a fixed semiconductor stock (in this case, that of NVIDIA) based on the performance of its competitors, previous pricing, and volume of shares sold. A variety of statistical learning models will be used, ranging from standard regression techniques to more non-linear models like random forest learning and k-Nearest neighbors. 



# Loading Packages and Data

```{r setup, include=TRUE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidymodels)
library(ggplot2)
library(corrplot)
library(discrim)
library(ggthemes)
library(kableExtra)
library(yardstick)
library(visdat)
library(scales)
library(glmnet)
library(quantmod)
library(dygraphs)


tidymodels_prefer()
conflicted::conflicts_prefer(yardstick::rsq)
set.seed(3435)
```

This dataset is comprised of 1 year's worth ( 252 business / trading days ) of New York Stock Exchange data for twelve of the most popular semiconductor manufacturers: Advanced Micro Devices (AMD), Applied Materials Inc. (AMAT), ASML Holding N.V. (ASML), Broadcom Inc. (AVGO), Intel Corporation (INTC), Monolithic Power Systems Inc. (MPWR), Nvidia Corp. (NVDA), NXP Semiconductors NV (NXPI), On Semiconductor Corp. (ON), Qualcomm Inc. (QCOM), Taiwan Semiconductor Manufacturing Co. Ltd. (TSM), and Texas Instruments Inc (TXN).

All stock market data was obtained from [Yahoo Finance](https://finance.yahoo.com/). Each company's one-year historical stock data was individually pulled from Yahoo's historical data on April 12th, 2024. For example, AMD's stock prices were obtained by downloading the CSV file from [AMD's Historical Data page](https://finance.yahoo.com/quote/AMD/history/), which results in a dataframe with the following variables and entries:

```{r  eval = TRUE, include=TRUE, warnings=FALSE}
loadSymbols("AMD", from="2023-01-01")
AMD = data.frame(AMD)
chartSeries(AMD,TA=c(addVo(),addBBands(),addMACD()))
```



However, since one goal of this analysis is to test the affect of competitor's stock performance on a fixed GPU manufacturer's stock price, multiple CSV files must be stored into raw data. The easiest way to do this was to create a separate CSV file, with header columns renamed to both resolve variable name conflicts and to distinguish the data specific to certain stocks. This was simply done by adding the stock's symbol (i.e. AMD, INTC, etc.) to the beginning of the original variable name: 



This results in a seemingly large initial data frame that contains different 73 predictors and 252 entries (corresponding to the 252 days that the stock market is open throughout the fiscal year). Out of the 73 predictors is 1 date variable (which is formatted as a Date data type using `as.Date()` ), and 6 predictors for each of the 12 chosen semiconductor manufacturers.

```{r  eval = TRUE, include=TRUE, warnings=FALSE}
dim(AMD)
```

Fortunately, a quick analysis shows that there is no missing data among any of the CSV files downloaded. This is somewhat expected though, since stock market data is meant to be as publicly available as possible and the original features are fairly common metrics for financial institutions to collect.

```{r  eval = TRUE, include=TRUE, warnings=FALSE}
vis_miss(AMD)
```


# Exploratory Data Analysis

We examine the data in terms of the predictors that are given to us, and then see if there are any other possible metrics to analyze our stock prices by. First we wish to explain the relevance of each predictor in the initial data frame, though not all variables will be used in our predictive models due to a high correlation (for example, the previous day's closing price is heavily correlated to the current day's opening price). Next, we examine other possible methods to predict our stocks behavior by looking at both historical metrics and normalized metrics. 

## Describing the Predictors

* `Open`: 
* `Close`:
* `Low`:
* `High`:
* `Volume`:

For each of the 12 semiconductor manufacturers chosen (AMAT, AMD, ASML, AVGO, INTC, MPWR, NVDA, NXPI, ON, QCOM, TSM, and TXN), there is a variable called xx_Open (where xx is one of the above stock symbols) which corresponds to that stock's opening price for the day. As the New York Stock exchange operates from 9:30AM to 4:00PM, this indicates the stock's price at 9:30AM that day. 

```{r include = TRUE, warning = FALSE, message = FALSE }
p <- dygraph(AMD[,c(1:4)], xlab = "Date", ylab = "Price", main = "AMD Price") %>%
  dySeries("AMD.Open", label = "Open", color = "black") %>%
  dySeries("AMD.Low", label = "Low", color = "red") %>%
  dySeries("AMD.High", label = "High", color = "green") %>%
  dySeries("AMD.Close", label = "Close", color = "orange") %>%
  dyRangeSelector() %>%
  dyCandlestick()%>%
  dyCrosshair(direction = "vertical") %>%
  dyHighlight(highlightCircleSize = 3, highlightSeriesBackgroundAlpha = 0.2, hideOnMouseOut = T)  %>%
  dyRoller(rollPeriod = 1)
p
```



 


 


## Data Correlation

While having a large array of predictors is in some sense useful for seeing the whole picture of the semiconductor market for the 2023-2024 fiscal year, there is also a potentially significant amount of unnecessary information. As mentioned prior, the behavior of many of our initial predictors coming from the CSV files are very closely related to one another — the closing price one day is directly tied to the opening price of the following day, and if a stock's minimum / Low value is increasing that generally means all 4 other predictors (aside from volume) are increasing as well. In addition, comparing the performance between two stocks is generally going to be heavily correlated due to the fact that they both follow the underlying market's climate.

Ultimately, in order to achieve a good understanding of the correlations between all of our predictors we will need to cross examine several subsets of our predictors to see which predictors are correlated for a single stock, and which predictors are useful for measuring competition between stocks. Dividing our correlation plots into two types, we first examine how the predictors are correlated for a **fixed** stock, and test this underlying trend accross a subset of our stocks (ASML, INTC, NVDA, and NXPI ):

```{r include = TRUE, warning = FALSE, message = FALSE, fig.show="hold", out.width="50%"}
select(AMD, is.numeric) %>%
  cor() %>%
  corrplot(method = "circle", type = "lower", diag = FALSE, tl.cex=0.6, title="INTC Correlation Plot")
```
 

 


# Setting Up Models

With a better picture in mind of how our stock prices can be measured from both the given metrics and how they interact with one another, we can now set up our data and begin training our models. This will be done in several steps, first preparing the data to ensure that our models do not become over-fitted to a particular data-set.


### Data Split

One of the primary ways we ensure robustness of our models is by partitioning our data into training and testing data. Foremost, this ensures that our model does not become overfit to the details and noise of our underlying data-set by introducing a portion of the data which is unseen during the training phase (i.e. the testing data). Ultimately, one would want outcome variable to have similar statistics / variance across both the training and testing sets — this is accomplished by *stratifying* our split about the desired outcome variable.


```{r eval = TRUE, include=TRUE, warnings=FALSE}
# SSD_split_1W <- initial_split(SSD, prop = 0.7,
#                                 strata = NVDA_avg_cl_1W)
# SSD_train_1W <- training(SSD_split_1W)
# SSD_test_1W <- testing(SSD_split_1W)
# 
# 
# SSD_split_2W <- initial_split(SSD, prop = 0.7,
#                                 strata = NVDA_avg_cl_2W)
# SSD_train_2W <- training(SSD_split_2W)
# SSD_test_2W <- testing(SSD_split_2W)
# 
# 
# 
# SSD_split_1M <- initial_split(SSD, prop = 0.7,
#                                 strata = NVDA_avg_cl_1M)
# SSD_train_1M <- training(SSD_split_1M)
# SSD_test_1M <- testing(SSD_split_1M)
# 
# 
# 
# SSD_split_2M <- initial_split(SSD, prop = 0.7,
#                                 strata = NVDA_avg_cl_2M)
# SSD_train_2M <- training(SSD_split_2M)
# SSD_test_2M <- testing(SSD_split_2M)
```


## One-Week Model Fitting

```{r eval = TRUE, include=TRUE, warnings=FALSE}
 # SSD_recipe_1W = recipe(
 #   NVDA_avg_cl_1W ~ NVDA_std_dev_cl_1W + NVDA_avg_ret_1W + NVDA_std_dev_ret_1W + NVDA_avg_vol_1W + NVDA_std_dev_vol_1W +
 #     TSM_avg_cl_1W + TSM_std_dev_cl_1W + TSM_avg_ret_1W + TSM_std_dev_ret_1W + TSM_avg_vol_1W + TSM_std_dev_vol_1W +
 #     NXPI_avg_cl_1W + NXPI_std_dev_cl_1W + NXPI_avg_ret_1W + NXPI_std_dev_ret_1W + NXPI_avg_vol_1W + NXPI_std_dev_vol_1W + QCOM_avg_cl_1W + QCOM_std_dev_cl_1W + QCOM_avg_ret_1W + QCOM_std_dev_ret_1W + QCOM_avg_vol_1W + QCOM_std_dev_vol_1W + MPWR_avg_cl_1W + MPWR_std_dev_cl_1W + MPWR_avg_ret_1W + MPWR_std_dev_ret_1W + MPWR_avg_vol_1W + MPWR_std_dev_vol_1W + ON_avg_cl_1W + ON_std_dev_cl_1W + ON_avg_ret_1W + ON_std_dev_ret_1W + ON_avg_vol_1W + ON_std_dev_vol_1W + AMD_avg_cl_1W + AMD_std_dev_cl_1W + AMD_avg_ret_1W + AMD_std_dev_ret_1W + AMD_avg_vol_1W + AMD_std_dev_vol_1W + INTC_avg_cl_1W + INTC_std_dev_cl_1W + INTC_avg_ret_1W + INTC_std_dev_ret_1W + INTC_avg_vol_1W + INTC_std_dev_vol_1W + AVGO_avg_cl_1W + AVGO_std_dev_cl_1W + AVGO_avg_ret_1W + AVGO_std_dev_ret_1W + AVGO_avg_vol_1W + AVGO_std_dev_vol_1W + ASML_avg_cl_1W + ASML_std_dev_cl_1W + ASML_avg_ret_1W + ASML_std_dev_ret_1W + ASML_avg_vol_1W + ASML_std_dev_vol_1W + AMAT_avg_cl_1W + AMAT_std_dev_cl_1W + AMAT_avg_ret_1W + AMAT_std_dev_ret_1W + AMAT_avg_vol_1W + AMAT_std_dev_vol_1W + TXN_avg_cl_1W + TXN_std_dev_cl_1W + TXN_avg_ret_1W + TXN_std_dev_ret_1W + TXN_avg_vol_1W + TXN_std_dev_vol_1W,
 #                     data=SSD_train_1W) %>%
 #  step_center(all_predictors()) %>% 
 #  step_scale(all_predictors())
 # 
 # SSD_recipe_2W = recipe(
 #   NVDA_avg_cl_2W ~ NVDA_std_dev_cl_2W + NVDA_avg_ret_2W + NVDA_std_dev_ret_2W + NVDA_avg_vol_2W + NVDA_std_dev_vol_2W +
 #     TSM_avg_cl_2W + TSM_std_dev_cl_2W + TSM_avg_ret_2W + TSM_std_dev_ret_2W + TSM_avg_vol_2W + TSM_std_dev_vol_2W +
 #     NXPI_avg_cl_2W + NXPI_std_dev_cl_2W + NXPI_avg_ret_2W + NXPI_std_dev_ret_2W + NXPI_avg_vol_2W + NXPI_std_dev_vol_2W + QCOM_avg_cl_2W + QCOM_std_dev_cl_2W + QCOM_avg_ret_2W + QCOM_std_dev_ret_2W + QCOM_avg_vol_2W + QCOM_std_dev_vol_2W + MPWR_avg_cl_2W + MPWR_std_dev_cl_2W + MPWR_avg_ret_2W + MPWR_std_dev_ret_2W + MPWR_avg_vol_2W + MPWR_std_dev_vol_2W + ON_avg_cl_2W + ON_std_dev_cl_2W + ON_avg_ret_2W + ON_std_dev_ret_2W + ON_avg_vol_2W + ON_std_dev_vol_2W + AMD_avg_cl_2W + AMD_std_dev_cl_2W + AMD_avg_ret_2W + AMD_std_dev_ret_2W + AMD_avg_vol_2W + AMD_std_dev_vol_2W + INTC_avg_cl_2W + INTC_std_dev_cl_2W + INTC_avg_ret_2W + INTC_std_dev_ret_2W + INTC_avg_vol_2W + INTC_std_dev_vol_2W + AVGO_avg_cl_2W + AVGO_std_dev_cl_2W + AVGO_avg_ret_2W + AVGO_std_dev_ret_2W + AVGO_avg_vol_2W + AVGO_std_dev_vol_2W + ASML_avg_cl_2W + ASML_std_dev_cl_2W + ASML_avg_ret_2W + ASML_std_dev_ret_2W + ASML_avg_vol_2W + ASML_std_dev_vol_2W + AMAT_avg_cl_2W + AMAT_std_dev_cl_2W + AMAT_avg_ret_2W + AMAT_std_dev_ret_2W + AMAT_avg_vol_2W + AMAT_std_dev_vol_2W + TXN_avg_cl_2W + TXN_std_dev_cl_2W + TXN_avg_ret_2W + TXN_std_dev_ret_2W + TXN_avg_vol_2W + TXN_std_dev_vol_2W,
 #                     data=SSD_train_2W) %>%
 #  step_center(all_predictors()) %>% 
 #  step_scale(all_predictors())
 # 
 #  SSD_recipe_1M = recipe(
 #   NVDA_avg_cl_1M ~ NVDA_std_dev_cl_1M + NVDA_avg_ret_1M + NVDA_std_dev_ret_1M + NVDA_avg_vol_1M + NVDA_std_dev_vol_1M +
 #     TSM_avg_cl_1M + TSM_std_dev_cl_1M + TSM_avg_ret_1M + TSM_std_dev_ret_1M + TSM_avg_vol_1M + TSM_std_dev_vol_1M +
 #     NXPI_avg_cl_1M + NXPI_std_dev_cl_1M + NXPI_avg_ret_1M + NXPI_std_dev_ret_1M + NXPI_avg_vol_1M + NXPI_std_dev_vol_1M + QCOM_avg_cl_1M + QCOM_std_dev_cl_1M + QCOM_avg_ret_1M + QCOM_std_dev_ret_1M + QCOM_avg_vol_1M + QCOM_std_dev_vol_1M + MPWR_avg_cl_1M + MPWR_std_dev_cl_1M + MPWR_avg_ret_1M + MPWR_std_dev_ret_1M + MPWR_avg_vol_1M + MPWR_std_dev_vol_1M + ON_avg_cl_1M + ON_std_dev_cl_1M + ON_avg_ret_1M + ON_std_dev_ret_1M + ON_avg_vol_1M + ON_std_dev_vol_1M + AMD_avg_cl_1M + AMD_std_dev_cl_1M + AMD_avg_ret_1M + AMD_std_dev_ret_1M + AMD_avg_vol_1M + AMD_std_dev_vol_1M + INTC_avg_cl_1M + INTC_std_dev_cl_1M + INTC_avg_ret_1M + INTC_std_dev_ret_1M + INTC_avg_vol_1M + INTC_std_dev_vol_1M + AVGO_avg_cl_1M + AVGO_std_dev_cl_1M + AVGO_avg_ret_1M + AVGO_std_dev_ret_1M + AVGO_avg_vol_1M + AVGO_std_dev_vol_1M + ASML_avg_cl_1M + ASML_std_dev_cl_1M + ASML_avg_ret_1M + ASML_std_dev_ret_1M + ASML_avg_vol_1M + ASML_std_dev_vol_1M + AMAT_avg_cl_1M + AMAT_std_dev_cl_1M + AMAT_avg_ret_1M + AMAT_std_dev_ret_1M + AMAT_avg_vol_1M + AMAT_std_dev_vol_1M + TXN_avg_cl_1M + TXN_std_dev_cl_1M + TXN_avg_ret_1M + TXN_std_dev_ret_1M + TXN_avg_vol_1M + TXN_std_dev_vol_1M,
 #                     data=SSD_train_1M) %>%
 #  step_center(all_predictors()) %>% 
 #  step_scale(all_predictors())
 #  
 #   SSD_recipe_2M = recipe(
 #   NVDA_avg_cl_2M ~ NVDA_std_dev_cl_2M + NVDA_avg_ret_2M + NVDA_std_dev_ret_2M + NVDA_avg_vol_2M + NVDA_std_dev_vol_2M +
 #     TSM_avg_cl_2M + TSM_std_dev_cl_2M + TSM_avg_ret_2M + TSM_std_dev_ret_2M + TSM_avg_vol_2M + TSM_std_dev_vol_2M +
 #     NXPI_avg_cl_2M + NXPI_std_dev_cl_2M + NXPI_avg_ret_2M + NXPI_std_dev_ret_2M + NXPI_avg_vol_2M + NXPI_std_dev_vol_2M + QCOM_avg_cl_2M + QCOM_std_dev_cl_2M + QCOM_avg_ret_2M + QCOM_std_dev_ret_2M + QCOM_avg_vol_2M + QCOM_std_dev_vol_2M + MPWR_avg_cl_2M + MPWR_std_dev_cl_2M + MPWR_avg_ret_2M + MPWR_std_dev_ret_2M + MPWR_avg_vol_2M + MPWR_std_dev_vol_2M + ON_avg_cl_2M + ON_std_dev_cl_2M + ON_avg_ret_2M + ON_std_dev_ret_2M + ON_avg_vol_2M + ON_std_dev_vol_2M + AMD_avg_cl_2M + AMD_std_dev_cl_2M + AMD_avg_ret_2M + AMD_std_dev_ret_2M + AMD_avg_vol_2M + AMD_std_dev_vol_2M + INTC_avg_cl_2M + INTC_std_dev_cl_2M + INTC_avg_ret_2M + INTC_std_dev_ret_2M + INTC_avg_vol_2M + INTC_std_dev_vol_2M + AVGO_avg_cl_2M + AVGO_std_dev_cl_2M + AVGO_avg_ret_2M + AVGO_std_dev_ret_2M + AVGO_avg_vol_2M + AVGO_std_dev_vol_2M + ASML_avg_cl_2M + ASML_std_dev_cl_2M + ASML_avg_ret_2M + ASML_std_dev_ret_2M + ASML_avg_vol_2M + ASML_std_dev_vol_2M + AMAT_avg_cl_2M + AMAT_std_dev_cl_2M + AMAT_avg_ret_2M + AMAT_std_dev_ret_2M + AMAT_avg_vol_2M + AMAT_std_dev_vol_2M + TXN_avg_cl_2M + TXN_std_dev_cl_2M + TXN_avg_ret_2M + TXN_std_dev_ret_2M + TXN_avg_vol_2M + TXN_std_dev_vol_2M,
 #                     data=SSD_train_2M) %>%
 #  step_center(all_predictors()) %>% 
 #  step_scale(all_predictors())

```



## k-Fold Cross Validation

```{r eval = TRUE, include=TRUE, warnings=FALSE}
# SSD_folds_1W <- vfold_cv(SSD_train_1W, v = 10, strata = NVDA_avg_cl_1W)
# SSD_folds_2W <- vfold_cv(SSD_train_2W, v = 10, strata = NVDA_avg_cl_2W)
# SSD_folds_1M <- vfold_cv(SSD_train_1M, v = 10, strata = NVDA_avg_cl_1M)
# SSD_folds_2M <- vfold_cv(SSD_train_2M, v = 10, strata = NVDA_avg_cl_2M)
```



## Fitting the Models

```{r eval = TRUE, include=TRUE, warnings=FALSE}
# # Linear Regression
# lm_model <- linear_reg() %>% 
#   set_engine("lm")
# 
# 
# # Ridge Regression
# ridge_model <- linear_reg(mixture = 0, 
#                          penalty = tune()) %>% 
#   set_mode("regression") %>% 
#   set_engine("glmnet")
# 
# # Lasso Regression
# lasso_model <- linear_reg(mixture = 1, 
#                          penalty = tune()) %>% 
#   set_mode("regression") %>% 
#   set_engine("glmnet")
# 
# 
# # Elastic Net
# elastic_net_model <- linear_reg(mixture = tune(), 
#                               penalty = tune()) %>% 
#   set_mode("regression") %>%
#   set_engine("glmnet")
# 
# # k-Nearest Neighbors (k = 7)
# knn_model <- nearest_neighbor(neighbors = tune()) %>% 
#   set_engine("kknn") %>% 
#   set_mode("regression")
```


### Set Up Workflows 

```{r eval = TRUE, include=TRUE, warnings=FALSE}
# # Linear Regression Workflows 
# lm_wflow_1W <- workflow() %>% 
#   add_model(lm_model) %>% 
#   add_recipe(SSD_recipe_1W)
# lm_wflow_2W <- workflow() %>% 
#   add_model(lm_model) %>% 
#   add_recipe(SSD_recipe_2W)
# lm_wflow_1M <- workflow() %>% 
#   add_model(lm_model) %>% 
#   add_recipe(SSD_recipe_1M)
# lm_wflow_2M <- workflow() %>% 
#   add_model(lm_model) %>% 
#   add_recipe(SSD_recipe_2M)
# 
# # Ridge Regression Workflows
# ridge_wflow_1W <- workflow() %>% 
#   add_model(ridge_model) %>% 
#   add_recipe(SSD_recipe_1W)
# ridge_wflow_2W <- workflow() %>% 
#   add_model(ridge_model) %>% 
#   add_recipe(SSD_recipe_2W)
# ridge_wflow_1M <- workflow() %>% 
#   add_model(ridge_model) %>% 
#   add_recipe(SSD_recipe_1M)
# ridge_wflow_2M <- workflow() %>% 
#   add_model(ridge_model) %>% 
#   add_recipe(SSD_recipe_2M)
# 
# # Lasso Regression Workflows
# lasso_wflow_1W <- workflow() %>% 
#   add_model(lasso_model) %>% 
#   add_recipe(SSD_recipe_1W)
# lasso_wflow_2W <- workflow() %>% 
#   add_model(lasso_model) %>% 
#   add_recipe(SSD_recipe_2W)
# lasso_wflow_1M <- workflow() %>% 
#   add_model(lasso_model) %>% 
#   add_recipe(SSD_recipe_1M)
# lasso_wflow_2M <- workflow() %>% 
#   add_model(lasso_model) %>% 
#   add_recipe(SSD_recipe_2M)
# 
# # Elastic Net Workflows
# elastic_net_wflow_1W <- workflow() %>% 
#   add_model(elastic_net_model) %>% 
#   add_recipe(SSD_recipe_1W)
# elastic_net_wflow_2W <- workflow() %>% 
#   add_model(elastic_net_model) %>% 
#   add_recipe(SSD_recipe_2W)
# elastic_net_wflow_1M <- workflow() %>% 
#   add_model(elastic_net_model) %>% 
#   add_recipe(SSD_recipe_1M)
# elastic_net_wflow_2M <- workflow() %>% 
#   add_model(elastic_net_model) %>% 
#   add_recipe(SSD_recipe_2M)
# 
# # k-Nearest Neighbors Workflows
# knn_wflow_1W <- workflow() %>% 
#   add_model(knn_model) %>% 
#   add_recipe(SSD_recipe_1W)
# knn_wflow_2W <- workflow() %>% 
#   add_model(knn_model) %>% 
#   add_recipe(SSD_recipe_2W)
# knn_wflow_1M <- workflow() %>% 
#   add_model(knn_model) %>% 
#   add_recipe(SSD_recipe_1M)
# knn_wflow_2M <- workflow() %>% 
#   add_model(knn_model) %>% 
#   add_recipe(SSD_recipe_2M)
```



### Hyperparameter Tuning 


Set up Grids:
```{r eval = TRUE, include=TRUE, warnings=FALSE}
# # Grid for Ridge Regression and Lasso Regression
# no_mixture_grid <- grid_regular(penalty(range = c(0,1)), levels = 50)
# 
# # Grid for Elastic Net
# elastic_net_grid <- grid_regular(penalty(range = c(0, 1),
#                                      trans = identity_trans()),
#                         mixture(range = c(0, 1)),
#                              levels = 10)
# 
# # k-Nearest Neighbors Net 
# knn_grid <- grid_regular(neighbors(range = c(2,20)), levels = 10)
```




Tune Parameters for 1-Week Recipe
```{r eval = TRUE, include=TRUE, warnings=FALSE}
# 
# # Find optimal parameters for ridge regression
# ridge_tune_1W <- tune_grid(
#   ridge_wflow_1W,
#   resamples = SSD_folds_1W,
#   grid = no_mixture_grid
# )
# ridge_final_wflow_1W <- select_best(ridge_tune_1W, metric="rmse" ) %>%
#   finalize_workflow(x=ridge_wflow_1W)
# 
# # Find optimal parameters for lasso regression
# lasso_tune_1W <- tune_grid(
#   lasso_wflow_1W,
#   resamples = SSD_folds_1W,
#   grid = no_mixture_grid
# )
# lasso_final_wflow_1W <- select_best(lasso_tune_1W, metric="rmse") %>%
#   finalize_workflow(x=lasso_wflow_1W)
# 
# # Find optimal parameters for Elastic Net
# elastic_net_tune_1W <- tune_grid(
#   elastic_net_wflow_1W,
#   resamples = SSD_folds_1W,
#   grid = elastic_net_grid
# )
# elastic_net_final_wflow_1W <- select_best(elastic_net_tune_1W, metric = "rmse") %>% 
#   finalize_workflow(x=elastic_net_wflow_1W)
# 
# # Find optimal parameters for k-Nearest Neighbors
# knn_tune_1W <- tune_grid(
#     knn_wflow_1W,
#     resamples = SSD_folds_1W,
#     grid = knn_grid
# )
# knn_final_wflow_1W <- select_best(knn_tune_1W, metric = "rmse") %>%
#   finalize_workflow(x=knn_wflow_1W)
```



Tune Parameters for 2-Week Recipe
```{r eval = TRUE, include=TRUE, warnings=FALSE}
# 
# # Find optimal parameters for ridge regression
# ridge_tune_2W <- tune_grid(
#   ridge_wflow_2W,
#   resamples = SSD_folds_2W,
#   grid = no_mixture_grid
# )
# ridge_final_wflow_2W <- select_best(ridge_tune_2W, metric="rmse" ) %>%
#   finalize_workflow(x=ridge_wflow_2W)
# 
# # Find optimal parameters for lasso regression
# lasso_tune_2W <- tune_grid(
#   lasso_wflow_2W,
#   resamples = SSD_folds_2W,
#   grid = no_mixture_grid
# )
# lasso_final_wflow_2W <- select_best(lasso_tune_2W, metric="rmse") %>%
#   finalize_workflow(x=lasso_wflow_2W)
# 
# # Find optimal parameters for Elastic Net
# elastic_net_tune_2W <- tune_grid(
#   elastic_net_wflow_2W,
#   resamples = SSD_folds_2W,
#   grid = elastic_net_grid
# )
# elastic_net_final_wflow_2W <- select_best(elastic_net_tune_2W,  metric = "rmse") %>% 
#   finalize_workflow(x=elastic_net_wflow_2W)
# 
# # Find optimal parameters for k-Nearest Neighbors
# knn_tune_2W <- tune_grid(
#     knn_wflow_2W,
#     resamples = SSD_folds_2W,
#     grid = knn_grid
# )
# knn_final_wflow_2W <- select_best(knn_tune_2W, metric = "rmse") %>% 
#   finalize_workflow(x=knn_wflow_2W)
```


Tune Parameters for 1-Month Recipe
```{r eval = TRUE, include=TRUE, warnings=FALSE}
# # Find optimal parameters for ridge regression
# ridge_tune_1M <- tune_grid(
#   ridge_wflow_1M,
#   resamples = SSD_folds_1M,
#   grid = no_mixture_grid
# )
# ridge_final_wflow_1M <- select_best(ridge_tune_1M, metric="rmse" ) %>% 
#   finalize_workflow(x=ridge_wflow_1M)
#  
# # Find optimal parameters for lasso regression
# lasso_tune_1M <- tune_grid(
#   lasso_wflow_1M,
#   resamples = SSD_folds_1M,
#   grid = no_mixture_grid
# )
# lasso_final_wflow_1M <- select_best(lasso_tune_1M, metric="rmse") %>%
#   finalize_workflow(x=lasso_wflow_1M)
# 
# # Find optimal parameters for Elastic Net
# elastic_net_tune_1M <- tune_grid(
#   elastic_net_wflow_1M,
#   resamples = SSD_folds_1M,
#   grid = elastic_net_grid
# )
# elastic_net_final_wflow_1M <- select_best(elastic_net_tune_1M, metric = "rmse") %>% 
#   finalize_workflow(x=elastic_net_wflow_1M)
# 
# # Find optimal parameters for k-Nearest Neighbors
# knn_tune_1M <- tune_grid(
#     knn_wflow_1M,
#     resamples = SSD_folds_1M,
#     grid = knn_grid
# )
# knn_final_wflow_1M <- select_best(knn_tune_1M, metric = "rmse") %>%
#   finalize_workflow(x=knn_wflow_1M)
```


Tune Parameters for 2-Month Recipe
```{r eval = TRUE, include=TRUE, warnings=FALSE}
# # Find optimal parameters for ridge regression
# ridge_tune_2M <- tune_grid(
#   ridge_wflow_2M,
#   resamples = SSD_folds_2M,
#   grid = no_mixture_grid
# )
# ridge_final_wflow_2M <-  select_best(ridge_tune_2M, metric="rmse" ) %>%
#   finalize_workflow(x=ridge_wflow_2M)
# 
# # Find optimal parameters for lasso regression
# lasso_tune_2M <- tune_grid(
#   lasso_wflow_2M,
#   resamples = SSD_folds_2M,
#   grid = no_mixture_grid
# )
# lasso_final_wflow_2M <-select_best(lasso_tune_2M, metric="rmse") %>%
#   finalize_workflow(x=lasso_wflow_2M)
# 
# # Find optimal parameters for Elastic Net
# elastic_net_tune_2M <- tune_grid(
#   elastic_net_wflow_2M,
#   resamples = SSD_folds_2M,
#   grid = elastic_net_grid
# )
# elastic_net_final_wflow_2M <- select_best(elastic_net_tune_2M, metric = "rmse") %>%
#   finalize_workflow(x=elastic_net_wflow_2M)
# 
# 
# # Find optimal parameters for k-Nearest Neighbors
# knn_tune_2M <- tune_grid(
#     knn_wflow_2M,
#     resamples = SSD_folds_2M,
#     grid = knn_grid
# )
# knn_final_wflow_2M <- select_best(knn_tune_2M, metric = "rmse") %>%
#   finalize_workflow(x=knn_wflow_2M)
```

### Model Fitting


```{r eval = TRUE, include=TRUE, warnings=FALSE}
# # Linear Regression Fits
# lm_fit_1W <- fit(lm_wflow_1W, SSD_train_1W)
# lm_fit_2W <- fit(lm_wflow_2W, SSD_train_2W)
# lm_fit_1M <- fit(lm_wflow_1M, SSD_train_1M)
# lm_fit_2M <- fit(lm_wflow_2M, SSD_train_2M)
# 
# # Ridge Regression Fits
# ridge_fit_1W <- fit(ridge_final_wflow_1W, SSD_train_1W)
# ridge_fit_2W <- fit(ridge_final_wflow_2W, SSD_train_2W)
# ridge_fit_1M <- fit(ridge_final_wflow_1M, SSD_train_1M)
# ridge_fit_2M <- fit(ridge_final_wflow_2M, SSD_train_2M)
# 
# # Lasso Regression Fits
# lasso_fit_1W <- fit(lasso_final_wflow_1W, SSD_train_1W)
# lasso_fit_2W <- fit(lasso_final_wflow_2W, SSD_train_2W)
# lasso_fit_1M <- fit(lasso_final_wflow_1M, SSD_train_1M)
# lasso_fit_2M <- fit(lasso_final_wflow_2M, SSD_train_2M)
# 
# # Elastic Net Fits
# elastic_net_fit_1W <- fit(elastic_net_final_wflow_1W, SSD_train_1W)
# elastic_net_fit_2W <- fit(elastic_net_final_wflow_2W, SSD_train_2W)
# elastic_net_fit_1M <- fit(elastic_net_final_wflow_1M, SSD_train_1M)
# elastic_net_fit_2M <- fit(elastic_net_final_wflow_2M, SSD_train_2M)
# 
# # k-Nearest Neighbors Fit
# knn_fit_1W <- fit(knn_final_wflow_1W, SSD_train_1W)
# knn_fit_2W <- fit(knn_final_wflow_2W, SSD_train_2W)
# knn_fit_1M <- fit(knn_final_wflow_1M, SSD_train_1M)
# knn_fit_2M <- fit(knn_final_wflow_2M, SSD_train_2M)
```

### Model Results for predicting 1-Week Average of NVIDIA Stock Price



```{r eval = TRUE, include=TRUE, warnings=FALSE}
# # Linear Regression Training 
# lm_train_res_1W <- predict(lm_fit_1W, new_data = SSD_train_1W %>% select(-NVDA_avg_cl_1W))
# lm_train_res_1W <- bind_cols(lm_train_res_1W, SSD_train_1W %>% select(NVDA_avg_cl_1W))
# 
# lm_train_res_2W <- predict(lm_fit_2W, new_data = SSD_train_2W %>% select(-NVDA_avg_cl_2W))
# lm_train_res_2W <- bind_cols(lm_train_res_2W, SSD_train_2W %>% select(NVDA_avg_cl_2W))
# 
# lm_train_res_1M <- predict(lm_fit_1M, new_data = SSD_train_1M %>% select(-NVDA_avg_cl_1M))
# lm_train_res_1M <- bind_cols(lm_train_res_1M, SSD_train_1M %>% select(NVDA_avg_cl_1M))
# 
# lm_train_res_2M <- predict(lm_fit_2M, new_data = SSD_train_2M %>% select(-NVDA_avg_cl_2M))
# lm_train_res_2M <- bind_cols(lm_train_res_2M, SSD_train_2M %>% select(NVDA_avg_cl_2M))
# 
# 
# # Ridge Regression Training
# ridge_train_res_1W <- predict(ridge_fit_1W, new_data = SSD_train_1W %>% select(-NVDA_avg_cl_1W))
# ridge_train_res_1W <- bind_cols(ridge_train_res_1W, SSD_train_1W %>% select(NVDA_avg_cl_1W))
# 
# ridge_train_res_2W <- predict(ridge_fit_2W, new_data = SSD_train_2W %>% select(-NVDA_avg_cl_2W))
# ridge_train_res_2W <- bind_cols(ridge_train_res_2W, SSD_train_2W %>% select(NVDA_avg_cl_2W))
# 
# ridge_train_res_1M <- predict(ridge_fit_1M, new_data = SSD_train_1M %>% select(-NVDA_avg_cl_1M))
# ridge_train_res_1M <- bind_cols(ridge_train_res_1M, SSD_train_1M %>% select(NVDA_avg_cl_1M))
# 
# ridge_train_res_2M <- predict(ridge_fit_2M, new_data = SSD_train_2M %>% select(-NVDA_avg_cl_2M))
# ridge_train_res_2M <- bind_cols(ridge_train_res_2M, SSD_train_2M %>% select(NVDA_avg_cl_2M))
# 
# 
# # Lasso Regression Training
# lasso_train_res_1W <- predict(lasso_fit_1W, new_data = SSD_train_1W %>% select(-NVDA_avg_cl_1W))
# lasso_train_res_1W <- bind_cols(lasso_train_res_1W, SSD_train_1W %>% select(NVDA_avg_cl_1W))
# 
# lasso_train_res_2W <- predict(lasso_fit_2W, new_data = SSD_train_2W %>% select(-NVDA_avg_cl_2W))
# lasso_train_res_2W <- bind_cols(lasso_train_res_2W, SSD_train_2W %>% select(NVDA_avg_cl_2W))
# 
# lasso_train_res_1M <- predict(lasso_fit_1M, new_data = SSD_train_1M %>% select(-NVDA_avg_cl_1M))
# lasso_train_res_1M <- bind_cols(lasso_train_res_1M, SSD_train_1M %>% select(NVDA_avg_cl_1M))
# 
# lasso_train_res_2M <- predict(lasso_fit_2M, new_data = SSD_train_2M %>% select(-NVDA_avg_cl_2M))
# lasso_train_res_2M <- bind_cols(lasso_train_res_2M, SSD_train_2M %>% select(NVDA_avg_cl_2M))
# 
# # Elastic Net Training
# elastic_net_train_res_1W <- predict(elastic_net_fit_1W, new_data = SSD_train_1W %>% select(-NVDA_avg_cl_1W))
# elastic_net_train_res_1W <- bind_cols(elastic_net_train_res_1W, SSD_train_1W %>% select(NVDA_avg_cl_1W))
# 
# elastic_net_train_res_2W <- predict(elastic_net_fit_2W, new_data = SSD_train_2W %>% select(-NVDA_avg_cl_2W))
# elastic_net_train_res_2W <- bind_cols(elastic_net_train_res_2W, SSD_train_2W %>% select(NVDA_avg_cl_2W))
# 
# elastic_net_train_res_1M <- predict(elastic_net_fit_1M, new_data = SSD_train_1M %>% select(-NVDA_avg_cl_1M))
# elastic_net_train_res_1M <- bind_cols(elastic_net_train_res_1M, SSD_train_1M %>% select(NVDA_avg_cl_1M))
# 
# elastic_net_train_res_2M <- predict(elastic_net_fit_2M, new_data = SSD_train_2M %>% select(-NVDA_avg_cl_2M))
# elastic_net_train_res_2M <- bind_cols(elastic_net_train_res_2M, SSD_train_2M %>% select(NVDA_avg_cl_2M))
# 
# # k-Nearest Neighbors Training
# knn_train_res_1W <- predict(knn_fit_1W, new_data = SSD_train_1W %>% select(-NVDA_avg_cl_1W))
# knn_train_res_1W <- bind_cols(knn_train_res_1W, SSD_train_1W %>% select(NVDA_avg_cl_1W))
# 
# knn_train_res_2W <- predict(knn_fit_2W, new_data = SSD_train_2W %>% select(-NVDA_avg_cl_2W))
# knn_train_res_2W <- bind_cols(knn_train_res_2W, SSD_train_2W %>% select(NVDA_avg_cl_2W))
# 
# knn_train_res_1M <- predict(knn_fit_1M, new_data = SSD_train_1M %>% select(-NVDA_avg_cl_1M))
# knn_train_res_1M <- bind_cols(knn_train_res_1M, SSD_train_1M %>% select(NVDA_avg_cl_1M))
# 
# knn_train_res_2M <- predict(knn_fit_2M, new_data = SSD_train_2M %>% select(-NVDA_avg_cl_2M))
# knn_train_res_2M <- bind_cols(knn_train_res_2M, SSD_train_2M %>% select(NVDA_avg_cl_2M))
```


## Model Accuracies


Root Mean Square Error (RMSE) results:
```{r eval = TRUE, include=TRUE, warnings=FALSE}
# tibble(Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", "Elastic Net", "k-Nearest Neighbors"),
#        One_Week = c((lm_train_res_1W %>% rmse( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (ridge_train_res_1W %>% rmse( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (lasso_train_res_1W %>% rmse( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (elastic_net_train_res_1W %>% rmse( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (knn_train_res_1W %>% rmse( NVDA_avg_cl_1W, .pred))$.estimate ),
#        Two_Week = c((lm_train_res_2W %>% rmse( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (ridge_train_res_2W %>% rmse( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (lasso_train_res_2W %>% rmse( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (elastic_net_train_res_2W %>% rmse( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (knn_train_res_2W %>% rmse( NVDA_avg_cl_2W, .pred))$.estimate),
#        One_Month = c((lm_train_res_1M %>% rmse( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (ridge_train_res_1M %>% rmse( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (lasso_train_res_1M %>% rmse( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (elastic_net_train_res_1M %>% rmse( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (knn_train_res_1M %>% rmse( NVDA_avg_cl_1M, .pred))$.estimate),
#        Two_Month = c((lm_train_res_2M %>% rmse( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (ridge_train_res_2M %>% rmse( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (lasso_train_res_2M %>% rmse( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (elastic_net_train_res_2M %>% rmse( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (knn_train_res_2M %>% rmse( NVDA_avg_cl_2M, .pred))$.estimate)
#        ) %>% 
#   kable() %>% 
#   kable_styling(full_width = F) %>% 
#   scroll_box(width = "100%", height = "200px")
```


R^2 results:
```{r eval = TRUE, include=TRUE, warnings=FALSE}

# tibble(Model = c("Linear Regression", "Ridge Regression", "Lasso Regression", "Elastic Net", "k-Nearest Neighbors"),
#        One_Week = c((lm_train_res_1W %>% rsq( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (ridge_train_res_1W %>% rsq( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (lasso_train_res_1W %>% rsq( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (elastic_net_train_res_1W %>% rsq( NVDA_avg_cl_1W, .pred))$.estimate,
#                     (knn_train_res_1W %>% rsq( NVDA_avg_cl_1W, .pred))$.estimate ),
#        Two_Week = c((lm_train_res_2W %>% rsq( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (ridge_train_res_2W %>% rsq( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (lasso_train_res_2W %>% rsq( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (elastic_net_train_res_2W %>% rsq( NVDA_avg_cl_2W, .pred))$.estimate,
#                     (knn_train_res_2W %>% rsq( NVDA_avg_cl_2W, .pred))$.estimate),
#        One_Month = c((lm_train_res_1M %>% rsq( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (ridge_train_res_1M %>% rsq( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (lasso_train_res_1M %>% rsq( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (elastic_net_train_res_1M %>% rsq( NVDA_avg_cl_1M, .pred))$.estimate,
#                     (knn_train_res_1M %>% rsq( NVDA_avg_cl_1M, .pred))$.estimate),
#        Two_Month = c((lm_train_res_2M %>% rsq( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (ridge_train_res_2M %>% rsq( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (lasso_train_res_2M %>% rsq( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (elastic_net_train_res_2M %>% rsq( NVDA_avg_cl_2M, .pred))$.estimate,
#                     (knn_train_res_2M %>% rsq( NVDA_avg_cl_2M, .pred))$.estimate)
#        ) %>% 
#   kable() %>% 
#   kable_styling(full_width = F) %>% 
#   scroll_box(width = "100%", height = "200px")
```